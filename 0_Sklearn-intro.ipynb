{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "68c18f9f-115f-490e-b81f-1b3794c428d0"
    }
   },
   "source": [
    "# Intro to Machine Learning\n",
    "\n",
    "## An oft-quoted definition\n",
    "> A computer program is said to learn from experience E with respect to some task T and some performance measure P if its performance on T, as measured by P, improves with experience E. \n",
    "\n",
    "> ( Mitchell 1997)\n",
    "\n",
    "Example Experiences: Supervised and Unsupervised learning\n",
    "\n",
    "Example Tasks: Classification, Regression, Clustering\n",
    "\n",
    "Example Performance: Accuracy, F1-Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Things you can do with scikit-learn\n",
    "[![ml-map](src/img/ml_map.png)](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "\n",
    "For a full list, check out the [User Guide](http://scikit-learn.org/stable/user_guide.html).\n",
    "\n",
    "## Further motivation\n",
    "![algo-comp](src/img/Model_comparison.jpg)\n",
    "_Olson 2017 https://arxiv.org/abs/1708.05070_\n",
    "\n",
    "\n",
    "### Example algorithms:\n",
    "* Linear regression, logistic regression\n",
    "* KNN\n",
    "* SVMs\n",
    "* Ensemble decision tree methods: Random Forests, Gradient boosted decision trees\n",
    "    * boosting vs bagging\n",
    "    * see the docs: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "* Naive Bayes (Gaussian, Multinomial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The whole process\n",
    "\n",
    "## Common API\n",
    "\n",
    "* Integrates well with other packages, eg. scipy sparse matrics (CSR, CSC), pandas DataFrames, visualization with matplotlib and seaborn\n",
    "\n",
    "### Bias-Variance tradeoff\n",
    "![bias-var](src/img/bias-variance.png)\n",
    "_from http://www.brnt.eu/phd/node14.html_\n",
    "\n",
    "Also see the example chapter from Jake VanderPlas [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Other Resources\n",
    "\n",
    "## Machine learning with scikit-learn\n",
    "Besides checking out the tutorials and examples that are part of scikit-learn's documentation I'd recommend:\n",
    "* Jake VanderPlas's book, [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/index.html#5.-Machine-Learning). All of the notebooks are also available through [Binder](https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb)\n",
    "* Browsing Kaggle [kernels](https://www.kaggle.com/kernels)\n",
    "\n",
    "## Books\n",
    "* Hastie, Elements of Statistical Learning\n",
    "* Bishop, Pattern Recognition and Machine Learning\n",
    "* Murphy, Machine Learning: a Probabilistic Perspective\n",
    "\n",
    "## Hyperparameter optimzation and AutoML\n",
    "* AutoML packages: [TPOT](https://github.com/rhiever/tpot), the [AutoML](https://github.com/automl) packages like [auto-sklearn](https://github.com/automl/auto-sklearn). These packages use genetic and bayesian optimization algorithms to evaluate the \"fitness\" or relationship between hyperparameter settings and model performance to search both across spaces where the relationship is uncertain as well as to focus in the subspaces that perform well. Can optimize not only the hyperparameters but also the type of model and preprocessing steps.\n",
    "* Bayesian optimization pacakges: [hyperopt](https://github.com/hyperopt/hyperopt), [Spearmint](https://github.com/HIPS/Spearmint), or [MOE](https://github.com/Yelp/MOE)\n",
    "\n",
    "## Other ML libraries in python\n",
    "* [XGBoost](https://github.com/dmlc/xgboost) or [LightGBM](https://github.com/Microsoft/LightGBM) for gradient boosting\n",
    "* MLlib for Spark\n",
    "\n",
    "## Deep learning\n",
    "* TensorFlow, PyTorch, MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allen]",
   "language": "python",
   "name": "conda-env-allen-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
